# Cubix Data Engineer Capstone - 1. R√©sz
## Python Projekt L√©trehoz√°sa Poetry-vel

**D√°tum:** 2024-12-15
**Szerz≈ë:** Yxonyx (kaiserjonatan911@gmail.com)

---

## üìã Tartalomjegyz√©k

1. [Projekt inicializ√°l√°s](#1-projekt-inicializ√°l√°s)
2. [Virtu√°lis k√∂rnyezet](#2-virtu√°lis-k√∂rnyezet)
3. [F√ºgg≈ës√©gek telep√≠t√©se](#3-f√ºgg≈ës√©gek-telep√≠t√©se)
4. [Projekt strukt√∫ra](#4-projekt-strukt√∫ra)
5. [Wheel Build √©s Databricks](#5-wheel-build-√©s-databricks)
6. [Verzi√≥kezel√©s (Git)](#6-verzi√≥kezel√©s-git)

---

## 1. Projekt inicializ√°l√°s

### Poetry konfigur√°l√°sa

```powershell
# Virtu√°lis k√∂rnyezet a projekt mapp√°ban legyen
poetry config virtualenvs.in-project true
```

### √öj projekt l√©trehoz√°sa

```powershell
poetry new cubix_data_engineer_capstone
cd cubix_data_engineer_capstone
```

---

## 2. Virtu√°lis k√∂rnyezet

### pyproject.toml

```toml
[project]
name = "cubix-data-engineer-capstone"
version = "0.2.24"
description = ""
authors = [
    {name = "Yxonyx", email="kaiserjonatan911@gmail.com"}
]
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
    "pyspark (>=3.5.4)",
    "numpy (>=1)"
]

[project.optional-dependencies]
delta = ["delta-spark>=3.3.0"]

[tool.poetry.group.dev.dependencies]
pytest = "^8.3.4"
pyarrow = "^19.0.0"
pre-commit = "^4.0.1"
pandas = "^2.2.3"

[tool.pytest.ini_options]
testpaths = ["./tests"]
filterwarnings = ["ignore:DeprecationWarning"]
addopts = "--disable-warnings -p no:warnings"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"
```

### Virtu√°lis k√∂rnyezet l√©trehoz√°sa

```powershell
python -m poetry install
```

---

## 3. F√ºgg≈ës√©gek telep√≠t√©se

### F≈ë f√ºgg≈ës√©gek

```powershell
python -m poetry add pyspark
python -m poetry add "numpy=^1"
```

### Dev f√ºgg≈ës√©gek

```powershell
python -m poetry add --group dev pytest pyarrow pre-commit pandas
```

### Csomagok √∂sszefoglal√≥ja

| Csomag | Verzi√≥ | T√≠pus | Le√≠r√°s |
|--------|--------|-------|--------|
| pyspark | >=3.5.4 | dependency | Apache Spark Python API |
| numpy | >=1 | dependency | Numerikus sz√°m√≠t√°sok |
| pytest | ^8.3.4 | dev | Tesztel√©si keretrendszer |
| pyarrow | ^19.0.0 | dev | Apache Arrow, Parquet |
| pre-commit | ^4.0.1 | dev | Git hook kezel√©s |
| pandas | ^2.2.3 | dev | DataFrame m≈±veletek |
| delta-spark | >=3.3.0 | optional | Delta Lake t√°mogat√°s |

---

## 4. Projekt strukt√∫ra

```
cubix_data_engineer_capstone/
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ deploy.yml           # CI/CD pipeline
‚îú‚îÄ‚îÄ .venv/                       # Virtu√°lis k√∂rnyezet
‚îú‚îÄ‚îÄ cubix_data_engineer_capstone/
‚îÇ   ‚îú‚îÄ‚îÄ etl/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py          # ETL modulok
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ authentication.py    # Spark session
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py            # Konfigur√°ci√≥
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ databricks.py        # UC Volume olvas√°s
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ datalake.py          # Data Lake m≈±veletek
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ upload_latest_whl.ps1    # Wheel felt√∂lt√©s script
‚îú‚îÄ‚îÄ dist/                        # Build kimenet
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ kurzus_1_projekt_letrehozas.md
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ .pre-commit-config.yaml
‚îú‚îÄ‚îÄ poetry.lock
‚îú‚îÄ‚îÄ pyproject.toml
‚îî‚îÄ‚îÄ README.md
```

---

## 5. Wheel Build √©s Databricks

### Verzi√≥ friss√≠t√©se

```powershell
python -m poetry version patch
# Bumping version from 0.2.23 to 0.2.24
```

### Wheel √©p√≠t√©se

```powershell
python -m poetry build -f wheel
# Built cubix_data_engineer_capstone-0.2.24-py3-none-any.whl
```

A wheel f√°jl a `dist/` mapp√°ban j√∂n l√©tre.

### Databricks telep√≠t√©s

1. T√∂ltsd fel a `.whl` f√°jlt Databricks Workspace-be
2. Notebook-ban:

```python
!pip install /Workspace/Users/majomkaiser@icloud.com/cubix_data_engineer_capstone-0.2.24-py3-none-any.whl
```

3. Kernel √∫jraind√≠t√°s (ha sz√ºks√©ges):

```python
%restart_python
```

4. Import √©s haszn√°lat:

```python
from cubix_data_engineer_capstone.utils.databricks import read_file_from_volume

full_path = "/Volumes/source_system/source_system/source_files/calendar/calendar.csv"
df = read_file_from_volume(full_path, "csv")
display(df)
```

### A `read_file_from_volume` f√ºggv√©ny

```python
from pyspark.sql import DataFrame, SparkSession


def read_file_from_volume(full_path: str, format: str) -> DataFrame:
    """Reads a file from UC Volume and returns it as a Spark DataFrame.

    :param full_path:   The path to the file on the volume.
    :param format:      The format of the file. Can be "csv", "parquet", "delta".
    :return:            DataFrame with the data.
    """
    if format not in ["csv", "parquet", "delta"]:
        raise ValueError(f"Invalid format: {format}. Supported formates are: csv, parquet, delta.")

    spark = SparkSession.getActiveSession()

    reader = spark.read.format(format)
    if format == "csv":
        reader = reader.option("header", "true")

    return reader.load(full_path)
```

---

## 6. Verzi√≥kezel√©s (Git)

### .gitignore (kivonatok)

```gitignore
__pycache__/
*.py[cod]
.venv/
dist/
*.egg-info/
.idea/
.vscode/
derby.log
metastore_db/
```

### Git parancsok

```powershell
git add -A
git commit -m "Add wheel build and Databricks integration"
git push origin main
```

---

## ‚úÖ Ellen≈ërz≈ë lista

- [x] Poetry konfigur√°ci√≥
- [x] Projekt l√©trehoz√°s
- [x] Virtu√°lis k√∂rnyezet (.venv)
- [x] F√ºgg≈ës√©gek telep√≠t√©se
- [x] Projekt strukt√∫ra kialak√≠t√°sa
- [x] Utils modulok (databricks.py, datalake.py, stb.)
- [x] Wheel build (`poetry build -f wheel`)
- [x] Databricks telep√≠t√©s √©s tesztel√©s
- [x] GitHub push

---

## üîó Hasznos linkek

- [Poetry dokument√°ci√≥](https://python-poetry.org/docs/)
- [PySpark dokument√°ci√≥](https://spark.apache.org/docs/latest/api/python/)
- [GitHub repo](https://github.com/Yxonyx/cubix_data_engineer_capstone)

---

**K√∂vetkez≈ë r√©sz:** ETL pipeline fejleszt√©s
