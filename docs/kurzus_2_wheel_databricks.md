# Cubix Data Engineer Capstone - 2. R√©sz
## Python Wheel √©s Databricks Integr√°ci√≥

**D√°tum:** 2024-12-16
**Szerz≈ë:** Yxonyx (kaiserjonatan911@gmail.com)

---

## üìã Tartalomjegyz√©k

1. [Mi a probl√©ma amit megoldunk?](#1-mi-a-probl√©ma-amit-megoldunk)
2. [A megold√°s: Python Wheel](#2-a-megold√°s-python-wheel)
3. [Projekt strukt√∫ra](#3-projekt-strukt√∫ra)
4. [A databricks.py modul](#4-a-databrickspy-modul)
5. [Build √©s Deploy folyamat](#5-build-√©s-deploy-folyamat)
6. [Haszn√°lat Databricks-ben](#6-haszn√°lat-databricks-ben)

---

## 1. Mi a probl√©ma amit megoldunk?

### A klasszikus probl√©ma

Databricks notebook-okban gyakran **ugyanazt a k√≥dot m√°solgatjuk** notebook-r√≥l notebook-ra:
- Spark session kezel√©s
- F√°jl olvas√°s/√≠r√°s Volume-okr√≥l
- Konfigur√°ci√≥k
- Seg√©df√ºggv√©nyek

**Probl√©m√°k:**
- ‚ùå K√≥d duplik√°ci√≥
- ‚ùå Neh√©z karbantart√°s
- ‚ùå Nincs verzi√≥kezel√©s
- ‚ùå Tesztelhet≈ës√©g hi√°nya

### A megold√°s

**Python wheel csomag** k√©sz√≠t√©se, amit:
- ‚úÖ Lok√°lisan fejleszt√ºnk (VS Code, PyCharm)
- ‚úÖ Verzi√≥kezelj√ºk (Git/GitHub)
- ‚úÖ Tesztel√ºnk (pytest)
- ‚úÖ Telep√≠t√ºnk Databricks-be

---

## 2. A megold√°s: Python Wheel

### Mi az a Wheel?

A **wheel** (`.whl`) a Python szabv√°nyos csomag form√°tuma:
- T√∂m√∂r√≠tett f√°jl (val√≥j√°ban ZIP)
- Tartalmazza a Python k√≥dot
- Telep√≠thet≈ë `pip install` paranccsal

### Mi√©rt j√≥ ez?

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     wheel      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   VS Code       ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫   ‚îÇ   Databricks    ‚îÇ
‚îÇ   (fejleszt√©s)  ‚îÇ    .whl        ‚îÇ   (futtat√°s)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

1. **Lok√°lisan fejlesztesz** ‚Üí IntelliSense, debugging, git
2. **Buildelsz** ‚Üí `poetry build -f wheel`
3. **Felt√∂lt√∂d** ‚Üí Databricks Workspace
4. **Telep√≠ted** ‚Üí `!pip install *.whl`
5. **Haszn√°lod** ‚Üí `from mypackage import ...`

---

## 3. Projekt strukt√∫ra

```
cubix_data_engineer_capstone/
‚îú‚îÄ‚îÄ cubix_data_engineer_capstone/     # F≈ë csomag
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ etl/                          # ETL logika
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ utils/                        # Seg√©df√ºggv√©nyek
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ authentication.py         # Spark session
‚îÇ       ‚îú‚îÄ‚îÄ config.py                 # Konfigur√°ci√≥
‚îÇ       ‚îú‚îÄ‚îÄ databricks.py             # UC Volume m≈±veletek
‚îÇ       ‚îî‚îÄ‚îÄ datalake.py               # Data Lake m≈±veletek
‚îú‚îÄ‚îÄ dist/                             # Build kimenet
‚îÇ   ‚îî‚îÄ‚îÄ *.whl                         # Wheel f√°jlok
‚îú‚îÄ‚îÄ docs/                             # Dokument√°ci√≥
‚îú‚îÄ‚îÄ tests/                            # Tesztek
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ pyproject.toml                    # Projekt konfig
‚îî‚îÄ‚îÄ poetry.lock                       # F√ºgg≈ës√©gek
```

---

## 4. A databricks.py modul

Ez a modul Unity Catalog Volume-okkal dolgozik.

### read_file_from_volume

F√°jl beolvas√°sa Volume-r√≥l DataFrame-be:

```python
from pyspark.sql import DataFrame, SparkSession


def read_file_from_volume(full_path: str, format: str) -> DataFrame:
    """Reads a file from UC Volume and returns it as a Spark DataFrame.

    :param full_path:   The path to the file on the volume.
    :param format:      The format of the file. Can be "csv", "parquet", "delta".
    :return:            DataFrame with the data.
    """
    if format not in ["csv", "parquet", "delta"]:
        raise ValueError(f"Invalid format: {format}")

    spark = SparkSession.getActiveSession()

    reader = spark.read.format(format)
    if format == "csv":
        reader = reader.option("header", "true")

    return reader.load(full_path)
```

### write_file_to_volume

DataFrame ment√©se Volume-ra:

```python
def write_file_to_volume(
        df: DataFrame,
        full_path: str,
        format: str,
        mode: str = "overwrite",
        partition_by: list[str] = None
) -> None:
    """Writes a DataFrame to UC Volume as parquet / csv / delta format.

    :param df:              DataFrame to be written.
    :param full_path:       The path to the file on the volume.
    :param format:          The format of the file.
    :param mode:            Write mode (default: "overwrite").
    :param partition_by:    List of columns to partition by.
    """
    if format not in ["csv", "parquet", "delta"]:
        raise ValueError(f"Invalid format: {format}")

    writer = df.write.mode(mode).format(format)
    if format == "csv":
        writer = writer.option("header", True)

    if partition_by:
        writer = writer.partitionBy(*partition_by)

    writer.save(full_path)
```

---

## 5. Build √©s Deploy folyamat

### 1. Verzi√≥ n√∂vel√©se

```powershell
python -m poetry version patch
# Bumping version from 0.2.24 to 0.2.25
```

Verzi√≥ t√≠pusok:
- `patch` ‚Üí 0.2.24 ‚Üí 0.2.25 (bug fix)
- `minor` ‚Üí 0.2.24 ‚Üí 0.3.0 (√∫j funkci√≥)
- `major` ‚Üí 0.2.24 ‚Üí 1.0.0 (breaking change)

### 2. Wheel √©p√≠t√©se

```powershell
python -m poetry build -f wheel
# Built cubix_data_engineer_capstone-0.2.25-py3-none-any.whl
```

A `.whl` f√°jl a `dist/` mapp√°ban j√∂n l√©tre.

### 3. Felt√∂lt√©s Databricks-be

1. Databricks Workspace megnyit√°sa
2. Jobb klikk ‚Üí Import ‚Üí File
3. V√°laszd ki a `.whl` f√°jlt
4. Import

### 4. Telep√≠t√©s notebook-ban

```python
!pip install /Workspace/Users/email@example.com/cubix_data_engineer_capstone-0.2.25-py3-none-any.whl
```

Ha friss√≠tesz:
```python
!pip install --force-reinstall /Workspace/Users/.../cubix_data_engineer_capstone-0.2.25-py3-none-any.whl
%restart_python
```

---

## 6. Haszn√°lat Databricks-ben

### Import

```python
from cubix_data_engineer_capstone.utils.databricks import read_file_from_volume, write_file_to_volume
```

### Olvas√°s

```python
full_path = "/Volumes/capstone/bronze/bronze/calendar/"
calendar_df = read_file_from_volume(full_path, "csv")
display(calendar_df)
```

### √çr√°s

```python
output_path = "/Volumes/capstone/silver/silver/calendar/"
write_file_to_volume(calendar_df, output_path, "parquet")
```

### Ctrl+Click

Ha a f√ºggv√©nyre **Ctrl+Click**-elsz, Databricks megmutatja a forr√°sk√≥dot - ez bizony√≠tja, hogy a csomag telep√≠tve van!

---

## üìä √ñsszefoglal√≥

| L√©p√©s | Parancs | Eredm√©ny |
|-------|---------|----------|
| Verzi√≥ n√∂vel√©s | `poetry version patch` | 0.2.24 ‚Üí 0.2.25 |
| Build | `poetry build -f wheel` | `.whl` f√°jl |
| Felt√∂lt√©s | Databricks Import | Workspace-be |
| Telep√≠t√©s | `!pip install *.whl` | Cluster-re |
| Haszn√°lat | `from ... import ...` | Notebook-ban |

---

## ‚úÖ Ellen≈ërz≈ë lista

- [x] Projekt strukt√∫ra kialak√≠t√°sa
- [x] `databricks.py` - read/write f√ºggv√©nyek
- [x] `poetry version patch` - verzi√≥ n√∂vel√©s
- [x] `poetry build -f wheel` - wheel √©p√≠t√©se
- [x] Felt√∂lt√©s Databricks Workspace-be
- [x] `!pip install` - telep√≠t√©s cluster-re
- [x] Tesztel√©s notebook-ban

---

## üîó Hasznos linkek

- [Poetry Build dokument√°ci√≥](https://python-poetry.org/docs/cli/#build)
- [Databricks Libraries](https://docs.databricks.com/libraries/index.html)
- [Unity Catalog Volumes](https://docs.databricks.com/data-governance/unity-catalog/volumes.html)

---

**El≈ëz≈ë r√©sz:** [1. Projekt l√©trehoz√°s](kurzus_1_projekt_letrehozas.md)
**K√∂vetkez≈ë r√©sz:** ETL Pipeline fejleszt√©s
